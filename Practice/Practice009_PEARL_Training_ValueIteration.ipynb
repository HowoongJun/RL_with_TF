{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame, sys, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_episodes = 3000\n",
    "dis = 0.9\n",
    "rList = []\n",
    "\n",
    "obstacleRadius = 10\n",
    "agentRadius = 10\n",
    "goalRadius = 20\n",
    "# get size of state and action from environment\n",
    "\n",
    "boundaryPos = [50, 50]\n",
    "boundaryLength = [200,200]\n",
    "initPosAgent = [boundaryPos[0] + boundaryLength[0] / 2 + 100, boundaryPos[1] + boundaryLength[1] / 2 + 50]\n",
    "initPosGoal =  [boundaryPos[0] + boundaryLength[0] / 2, boundaryPos[1] + boundaryLength[1] / 2]\n",
    "\n",
    "moveObstacles = False\n",
    "action_size = 9\n",
    "obsNumber = 4\n",
    "state_size = 2\n",
    "hidden_size = 128\n",
    "obsAngleUnit = 1\n",
    "hidden_size = 2\n",
    "learning_rate = 0.00000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_variable(scope_name, var_name, shape):\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        v = tf.get_variable(var_name, shape)\n",
    "        scope.reuse_variables()\n",
    "\n",
    "def get_scope_variable(scope_name, var_name):\n",
    "    with tf.variable_scope(scope_name, reuse=True):\n",
    "        v = tf.get_variable(var_name)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ckWall(xPos, yPos):\n",
    "    flagWall = 0\n",
    "    if(xPos < boundaryPos[0]):\n",
    "        xPos = boundaryPos[0]\n",
    "#         xPos = boundaryPos[0] + boundaryLength[0]\n",
    "        flagWall = -1\n",
    "    elif(xPos > boundaryPos[0] + boundaryLength[0]):\n",
    "        xPos = boundaryPos[0] + boundaryLength[0]\n",
    "#         xPos = boundaryPos[0]\n",
    "        flagWall = -1\n",
    "    if(yPos < boundaryPos[1]):\n",
    "        yPos = boundaryPos[1]\n",
    "#         yPos = boundaryPos[1] + boundaryLength[1]\n",
    "        flagWall = -1\n",
    "    elif(yPos > boundaryPos[1] + boundaryLength[1]):\n",
    "        yPos = boundaryPos[1] + boundaryLength[1]\n",
    "#         yPos = boundaryPos[1]\n",
    "        flagWall = -1\n",
    "    return [int(round(xPos)), int(round(yPos)), flagWall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def takeAction(action):\n",
    "    xAction = 0\n",
    "    yAction = 0\n",
    "    actionOffset = 2\n",
    "    if action == 0:\n",
    "        xAction = actionOffset\n",
    "    elif action == 1:\n",
    "        xAction = actionOffset\n",
    "        yAction = actionOffset\n",
    "    elif action == 2:\n",
    "        xAction = actionOffset\n",
    "        yAction = -actionOffset\n",
    "    elif action == 3:\n",
    "        xAction = -actionOffset\n",
    "        yAction = actionOffset\n",
    "    elif action == 4:\n",
    "        xAction = -actionOffset\n",
    "    elif action == 5:\n",
    "        xAction = -actionOffset\n",
    "        yAction = -actionOffset\n",
    "    elif action == 6:\n",
    "        yAction = -actionOffset\n",
    "    elif action == 7:\n",
    "        yAction = actionOffset\n",
    "    elif action == 8:\n",
    "        xAction = 0\n",
    "        yAction = 0\n",
    "        \n",
    "    return [xAction, yAction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stateGenerator(intenPref, distPref):\n",
    "    returnSum = [intenPref, distPref]\n",
    "    returnSum = np.reshape(returnSum, [1, state_size])\n",
    "#     print returnSum\n",
    "    return returnSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rangeFinder(allObsPos, rangeCenter):\n",
    "    allObsAgtDistance = [0 for _ in range(obsNumber)]\n",
    "    for i in range(0, obsNumber):\n",
    "        allObsAgtDistance[i] = math.sqrt((allObsPos[i][0] - rangeCenter[0])**2 + (allObsPos[i][1] - rangeCenter[1])**2)\n",
    "    index = np.argmin(allObsAgtDistance)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,  [None, state_size], name = \"input_x\")\n",
    "initialize_variable(\"scope1\", \"W1\", [2, 1])\n",
    "W1 = get_scope_variable(\"scope1\", \"W1\")\n",
    "initialize_variable(\"scope1\", \"W2\", [1, 128])\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "W2 = get_scope_variable(\"scope1\", \"W2\")\n",
    "initialize_variable(\"scope1\", \"W3\", [128, 1])\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2))\n",
    "W3 = get_scope_variable(\"scope1\", \"W3\")\n",
    "\n",
    "Qpred = tf.matmul(layer2, W3)\n",
    "Y = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Episode ', 0, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-3792.2212656\n",
      "('Episode ', 1, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-3062.90097906\n",
      "('Episode ', 2, 'Starts!')\n",
      "Goal Reached!25  \n",
      "700.353262998\n",
      "('Episode ', 3, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-3080.24434919\n",
      "('Episode ', 4, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-3134.79706617\n",
      "('Episode ', 5, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-5651.43550938\n",
      "('Episode ', 6, 'Starts!')\n",
      "('Collision!', 0)   \n",
      "-13106.3365979\n",
      "('Episode ', 7, 'Starts!')\n",
      "('Collision!', 0)      \n",
      "-17117.9872902\n",
      "('Episode ', 8, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-6552.11462491\n",
      "('Episode ', 9, 'Starts!')\n",
      "('Collision!', 0)  \n",
      "-6732.07118629\n",
      "('Episode ', 10, 'Starts!')\n",
      "('Collision!', 0) \n",
      "-6245.75171451\n",
      "('Episode ', 11, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-3531.35752341\n",
      "('Episode ', 12, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-7419.4884448\n",
      "('Episode ', 13, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-3502.96134657\n",
      "('Episode ', 14, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-7931.17858078\n",
      "('Episode ', 15, 'Starts!')\n",
      "('Collision!', 0)   \n",
      "-13523.4237442\n",
      "('Episode ', 16, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-3829.95138664\n",
      "('Episode ', 17, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-3454.06133331\n",
      "('Episode ', 18, 'Starts!')\n",
      "('Collision!', 0)  \n",
      "-13671.4669796\n",
      "('Episode ', 19, 'Starts!')\n",
      "('Collision!', 0)       \n",
      "-15514.5611246\n",
      "('Episode ', 20, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-4286.58705259\n",
      "('Episode ', 21, 'Starts!')\n",
      "('Collision!', 0)                      \n",
      "-36344.8662983\n",
      "('Episode ', 22, 'Starts!')\n",
      "('Collision!', 0)\n",
      "-9023.24892483\n",
      "('Episode ', 23, 'Starts!')\n",
      "('Collision!', 0)                                         \n",
      "-58441.5786983\n",
      "('Episode ', 24, 'Starts!')\n",
      "('Collision!', 0) \n",
      "-9392.08020476\n",
      "('Episode ', 25, 'Starts!')\n",
      "('Collision!', 0)   \n",
      "-11235.2273142\n",
      "('Episode ', 26, 'Starts!')\n",
      "('Collision!', 0)       \n",
      "-19247.5429912\n",
      "('Episode ', 27, 'Starts!')\n",
      "('Collision!', 0)               \n",
      "-34537.0721956\n",
      "('Episode ', 28, 'Starts!')\n",
      "('Collision!', 0)                                  \n",
      "-77688.0177983\n",
      "('Episode ', 29, 'Starts!')\n",
      "('Collision!', 0)           \n",
      "-22134.9996532\n",
      "('Episode ', 30, 'Starts!')\n",
      "('Collision!', 0)                       \n",
      "-47537.6533464\n",
      "('Episode ', 31, 'Starts!')\n",
      "('Collision!', 0)   \n",
      "-14633.8971456\n",
      "('Episode ', 32, 'Starts!')\n",
      "('Collision!', 0)                  \n",
      "-31397.9970501\n",
      "('Episode ', 33, 'Starts!')\n",
      "('Collision!', 0)                              \n",
      "-59110.2989354\n",
      "('Episode ', 34, 'Starts!')\n",
      "('Collision!', 0)                 \n",
      "-38230.9791187\n",
      "('Episode ', 35, 'Starts!')\n",
      "('Collision!', 0)                                                                                \n",
      "-154762.184399\n",
      "('Episode ', 36, 'Starts!')\n",
      "('Collision!', 0)                    \n",
      "-55163.8973273\n",
      "('Episode ', 37, 'Starts!')\n",
      "('Collision!', 0)                             \n",
      "-58530.9876288\n",
      "('Episode ', 38, 'Starts!')\n",
      "('Collision!', 0)                                                        \n",
      "-96068.4521047\n",
      "('Episode ', 39, 'Starts!')\n",
      "('Collision!', 0)            \n",
      "-20764.077734\n",
      "('Episode ', 40, 'Starts!')\n",
      "('Collision!', 0)                                                       \n",
      "-104438.272087\n",
      "('Episode ', 41, 'Starts!')\n",
      "('Collision!', 0)                                                                      \n",
      "-111220.67417\n",
      "('Episode ', 42, 'Starts!')\n",
      "('Collision!', 0)  \n",
      "-7679.09138738\n",
      "('Episode ', 43, 'Starts!')\n",
      "('Collision!', 0)                                                                                                         \n",
      "-183079.181373\n",
      "('Episode ', 44, 'Starts!')\n",
      "('Collision!', 0)                                  \n",
      "-56331.5955309\n",
      "('Episode ', 45, 'Starts!')\n",
      "('Collision!', 0)         \n",
      "-23014.555101\n",
      "('Episode ', 46, 'Starts!')\n",
      "('Collision!', 0)        \n",
      "-13457.4433604\n",
      "('Episode ', 47, 'Starts!')\n",
      "('Collision!', 0)                                                                                                   \n",
      "-163492.013885\n",
      "('Episode ', 48, 'Starts!')\n",
      "('Collision!', 0)                                                                            \n",
      "-165845.472264\n",
      "('Episode ', 49, 'Starts!')\n",
      "('Collision!', 0)                                                                                                                                                                                                                       \n",
      "-398492.564193\n",
      "('Episode ', 50, 'Starts!')\n",
      "('Collision!', 0)                                                                      \n",
      "-143077.683252\n",
      "('Episode ', 51, 'Starts!')\n",
      "('Collision!', 0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "-1352540.32292\n",
      "('Episode ', 52, 'Starts!')\n",
      "-0.758846493041              \r"
     ]
    }
   ],
   "source": [
    "#display\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode([300,300])\n",
    "screen.fill([200, 200, 200])\n",
    "\n",
    "episodeNo = []\n",
    "scorePlot = []\n",
    "\n",
    "obsAngleIdx = 0\n",
    "# Make Obstacles (obsNumber)\n",
    "obstaclePos = [[0, 0] for _ in range(obsNumber)]\n",
    "posOffset = 60\n",
    "obstaclePos[0][0] = int(initPosGoal[0] + posOffset) \n",
    "obstaclePos[0][1] = int(initPosGoal[1]) \n",
    "obstaclePos[1][0] = int(initPosGoal[0]) \n",
    "obstaclePos[1][1] = int(initPosGoal[1] + posOffset) \n",
    "obstaclePos[2][0] = int(initPosGoal[0] - posOffset) \n",
    "obstaclePos[2][1] = int(initPosGoal[1]) \n",
    "obstaclePos[3][0] = int(initPosGoal[0]) \n",
    "obstaclePos[3][1] = int(initPosGoal[1] - posOffset) \n",
    "beta = 0.0\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    Qs = np.zeros(action_size)\n",
    "#     Qs = [0 for _ in range(action_size)]\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        x = initPosAgent[0]\n",
    "        y = initPosAgent[1]\n",
    "        print(\"Episode \", episode, \"Starts!\")\n",
    "        \n",
    "        xMove = 0\n",
    "        yMove = 0\n",
    "        idx = rangeFinder(obstaclePos, [x,y])\n",
    "        FeatureVec1 = (x - initPosGoal[0])**2 + (y - initPosGoal[1])**2\n",
    "        FeatureVec2 = 100000.0 / (beta + (x - obstaclePos[idx][0])**2 + (y - obstaclePos[idx][1])**2)\n",
    "#         FeatureVec2 = (x - obstaclePos[idx][0])**2 + (y - obstaclePos[idx][1])**2\n",
    "\n",
    "        state = stateGenerator(FeatureVec1, FeatureVec2)\n",
    "        e = 1.0 / ((episode/100.0) + 1.0)\n",
    "#             e = 0.01\n",
    "#         e = 1.0 / (1.0 + episode / 100.0)\n",
    "        rAll = 0\n",
    "        step_count = 0\n",
    "        while not done:\n",
    "            step_count += 1\n",
    "            Qinput = np.reshape(state, [1, state_size])\n",
    "            \n",
    "            for i in range(0,action_size):\n",
    "                [tmpXMove, tmpYMove] = takeAction(i)\n",
    "                tmpX = x + tmpXMove\n",
    "                tmpY = y + tmpYMove\n",
    "                idx = rangeFinder(obstaclePos, [tmpX,tmpY])\n",
    "                FeatureVec1 = (tmpX - initPosGoal[0])**2 + (tmpY - initPosGoal[1])**2\n",
    "                FeatureVec2 = 100000.0 / (beta + (tmpX - obstaclePos[idx][0])**2 + (tmpY - obstaclePos[idx][1])**2)\n",
    "#                 FeatureVec2 = (tmpX - obstaclePos[idx][0])**2 + (tmpY - obstaclePos[idx][1])**2\n",
    "                tmpNextState = stateGenerator(FeatureVec1, FeatureVec2)\n",
    "                QinputPred = np.reshape(tmpNextState, [1, state_size])\n",
    "                Qs[i] = sess.run(Qpred, feed_dict = {X: QinputPred})\n",
    "#             print Qs\n",
    "#             print \"========================================\"\n",
    "            if np.random.rand(1) < e:\n",
    "                action = random.randrange(0,8)\n",
    "            else:\n",
    "                action = np.argmax(Qs)\n",
    "#             print action\n",
    "            xMove = 0\n",
    "            yMove = 0\n",
    "            [xMove, yMove] = takeAction(action)\n",
    "\n",
    "            x = x + xMove\n",
    "            y = y + yMove\n",
    "            \n",
    "            wallFlag = 0\n",
    "            collisionFlag = 0\n",
    "            [x, y, wallFlag] = ckWall(x, y)\n",
    "            pygame.draw.circle(screen, [100,100,255],[x,y],agentRadius,0)\n",
    "            idx = rangeFinder(obstaclePos, [x,y])\n",
    "            FeatureVec1 = (x - initPosGoal[0])**2 + (y - initPosGoal[1])**2\n",
    "            FeatureVec2 = 100000.0 / (beta + (tmpX - obstaclePos[idx][0])**2 + (tmpY - obstaclePos[idx][1])**2)\n",
    "#             FeatureVec2 = (x - obstaclePos[idx][0])**2 + (y - obstaclePos[idx][1])**2\n",
    "            next_state = stateGenerator(FeatureVec1, FeatureVec2)\n",
    "            ## Draw Environment (Obstacle + Map)\n",
    "            if(math.sqrt((x - initPosGoal[0])**2 + (y - initPosGoal[1])**2) <= goalRadius + agentRadius):\n",
    "                print(\"Goal Reached!\")\n",
    "                reward = 2000.0\n",
    "                collisionFlag = 1\n",
    "                done = True\n",
    "            for i in range(0,obsNumber):\n",
    "                pygame.draw.circle(screen, [255, 50, 50], obstaclePos[i], obstacleRadius, 0)\n",
    "                if math.sqrt((x - obstaclePos[i][0])**2 + (y - obstaclePos[i][1])**2) <= obstacleRadius + agentRadius:\n",
    "                    print(\"Collision!\", idx)\n",
    "                    reward = -3000.0\n",
    "                    collisionFlag = -1\n",
    "                    done = True\n",
    "                    break\n",
    "            Qinput1 = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            if not done:\n",
    "                reward = -7.0 * math.sqrt((x - initPosGoal[0])**2 + (y - initPosGoal[1])**2) / 1000.0\n",
    "#                 reward = 0\n",
    "                QcurrPred = reward + dis * Qs[action]\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                episodeNo = episodeNo + [episode]\n",
    "                if collisionFlag == 1:\n",
    "                    QcurrPred = 2000.0\n",
    "                    rList.append(1)\n",
    "                elif collisionFlag == -1:\n",
    "                    QcurrPred = -3000.0\n",
    "                    rList.append(0)\n",
    "#             Qs1 = sess.run(Qpred, feed_dict = {X: Qinput1})\n",
    "                print score\n",
    "                scorePlot = scorePlot + [score]\n",
    "#             QcurrPred = reward + dis * np.max(Qs)\n",
    "\n",
    "            print QcurrPred, '\\r',\n",
    "            sess.run(train, feed_dict={X: Qinput1, Y: QcurrPred})\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                for i in range(0,obsNumber):\n",
    "                    pygame.draw.circle(screen, [255, 50, 50], obstaclePos[i], obstacleRadius, 0)\n",
    "                if episode % 100 == 0:\n",
    "                    plt.plot(episodeNo, scorePlot, linewidth = 0.3)\n",
    "                    plt.savefig(\"/home/howoongjun/RLPractice/Practice/DataSave/RewardPlot.png\", dpi=300)\n",
    "            pygame.draw.circle(screen, [100,255,100], initPosGoal, goalRadius, 1)\n",
    "            pygame.draw.rect(screen, [255,100,100],[boundaryPos[0],boundaryPos[1], boundaryLength[0], boundaryLength[1]],1)\n",
    "            pygame.display.flip()\n",
    "            screen.fill([200,200,200])\n",
    "#             print sess.run(loss, feed_dict = {X:Qinput, Y:QcurrPred}), '\\r',\n",
    "#             print sess.run(Qpred, feed_dict = {X: QinputPred}), '\\r',\n",
    "            \n",
    "    print sess.run(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
