{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pygame, sys, random\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "num_episodes = 30001\n",
    "\n",
    "agentRadius = 10\n",
    "\n",
    "# get size of state and action from environment\n",
    "\n",
    "boundaryPos = [400, 590]\n",
    "boundaryLength = [70,70]\n",
    "initPosAgent = [boundaryPos[0] + boundaryLength[0] / 2, boundaryPos[1] + boundaryLength[1] / 2]\n",
    "goalPos = [boundaryPos[0] + boundaryLength[0], boundaryPos[1]]\n",
    "\n",
    "moveObstacles = True\n",
    "action_size = 9\n",
    "obsNumber = 1\n",
    "state_size = obsNumber * 2 + 1\n",
    "# state_size = obsNumber + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A2C(Advantage Actor-Critic) agent\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.load_model = True\n",
    "        \n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.00002\n",
    "        self.critic_lr = 0.00005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"./Practice004_DataSave/Actor.h5\")\n",
    "            self.critic.load_weights(\"./Practice004_DataSave/Critic.h5\")\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax', kernel_initializer='glorot_normal'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        critic.add(Dense(self.value_size, activation='linear', kernel_initializer='glorot_normal'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "#         print policy\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ckWall(xPos, yPos):\n",
    "    flagWall = 0\n",
    "    if(xPos < boundaryPos[0]):\n",
    "        xPos = boundaryPos[0]\n",
    "        flagWall = -1\n",
    "    elif(xPos > boundaryPos[0] + boundaryLength[0]):\n",
    "        xPos = boundaryPos[0] + boundaryLength[0]\n",
    "        flagWall = -1\n",
    "    if(yPos < boundaryPos[1]):\n",
    "        yPos = boundaryPos[1]\n",
    "        flagWall = -1\n",
    "    elif(yPos > boundaryPos[1] + boundaryLength[1]):\n",
    "        yPos = boundaryPos[1] + boundaryLength[1]\n",
    "        flagWall = -1\n",
    "        \n",
    "    return [xPos, yPos, flagWall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ckInit(initLoc, obsLoc):\n",
    "#     if math.sqrt((initLoc[0] - obsLoc[0])**2 + (initLoc[1] - obsLoc[1])**2) <= 2*agentRadius + 1:\n",
    "#         if initLoc[0] - obsLoc[0] < 0:\n",
    "#             obsLoc[0] += 1\n",
    "#         else:\n",
    "#             obsLoc[0] -= 1\n",
    "#         if initLoc[1] - obsLoc[1] < 0:\n",
    "#             obsLoc[1] += 1\n",
    "#         else:\n",
    "#             obsLoc[1] -= 1\n",
    "    # Avoid interrupting goal position\n",
    "    if math.sqrt((goalPos[0] - obsLoc[0])**2 + (goalPos[1] - obsLoc[1])**2) <= 2*agentRadius + 1:\n",
    "        obsLoc[0] -= 1\n",
    "        obsLoc[1] += 1\n",
    "    return obsLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stateGenerator(obsPosition, agtPosition):\n",
    "    returnSum = []\n",
    "    for i in range(0,obsNumber):\n",
    "#         returnSum = returnSum + [math.sqrt((agtPosition[0] - obsPosition[i][0])**2 + (agtPosition[1] - obsPosition[i][1])**2)]\n",
    "        returnSum = returnSum + [agtPosition[0] - obsPosition[i][0], agtPosition[1] - obsPosition[i][1]]\n",
    "#     returnSum = returnSum + [agtPosition[0] - 640, agtPosition[1] - 450]\n",
    "    returnSum = returnSum + [math.sqrt((agtPosition[0] - goalPos[0])**2 + (agtPosition[1] - goalPos[1])**2)]\n",
    "    returnSum = np.reshape(returnSum, [1, state_size])\n",
    "    return returnSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def takeAction(action):\n",
    "    xAction = 0\n",
    "    yAction = 0\n",
    "    if action == 0:\n",
    "        xAction = 1\n",
    "    elif action == 1:\n",
    "        xAction = 1\n",
    "        yAction = 1\n",
    "    elif action == 2:\n",
    "        xAction = 1\n",
    "        yAction = -1            \n",
    "    elif action == 3:\n",
    "        xAction = -1\n",
    "        yAction = 1\n",
    "    elif action == 4:\n",
    "        xAction = -1\n",
    "    elif action == 5:\n",
    "        xAction = -1\n",
    "        yAction = -1\n",
    "    elif action == 6:\n",
    "        yAction = -1\n",
    "    elif action == 7:\n",
    "        yAction = 1\n",
    "    elif action == 8:\n",
    "        xAction = 0\n",
    "        yAction = 0\n",
    "        \n",
    "    return [xAction, yAction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-0bf228c0de26>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-0bf228c0de26>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    if obstaclePos[i][0] <= goalPos[0] - agentRadius or            if obstaclePos[i][1] >= goalPos[1] + agentRadius or obstaclePos[i][1] <= goalPos[1] - agentRadius:\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "screen = pygame.display.set_mode([1280,960])\n",
    "screen.fill([200, 200, 200])\n",
    "\n",
    "# make A2C agent\n",
    "agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "rList, episodes = [], []\n",
    "\n",
    "# Make Obstacles (obsNumber)\n",
    "obstaclePos = [[0, 0] for _ in range(obsNumber)]\n",
    "for i in range(0,obsNumber):\n",
    "    while True:\n",
    "        obstaclePos[i][0] = boundaryPos[0] + random.randrange(1, boundaryLength[0])\n",
    "        obstaclePos[i][1] = boundaryPos[1] + random.randrange(1, boundaryLength[1])\n",
    "        if obstaclePos[i][0] <= goalPos[0] - agentRadius or obstaclePos[i][0] >= goalPos[0] + agentRadius:\n",
    "            if obstaclePos[i][1] >= goalPos[1] + agentRadius or obstaclePos[i][1] <= goalPos[1] - agentRadius:\n",
    "                if obstaclePos[i][0] <= initPosAgent[0] - agentRadius or obstaclePos[i][0] >= initPosAgent[0] + agentRadius:\n",
    "                    if obstaclePos[i][1] >= initPosAgent[1] + agentRadius or obstaclePos[i][1] <= initPosAgent[1] - agentRadius:\n",
    "                        break\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    # Initialize\n",
    "    done = False\n",
    "    score = 0\n",
    "    x = initPosAgent[0]#boundaryPos[0]\n",
    "    y = initPosAgent[1]#boundaryPos[1]\n",
    "    print(\"Episode \", e, \"Starts!\")\n",
    "    state = stateGenerator(obstaclePos, [x,y])\n",
    "    #state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        xMove = 0\n",
    "        yMove = 0\n",
    "        \n",
    "        [xMove, yMove] = takeAction(action)\n",
    "        \n",
    "        x = x + xMove\n",
    "        y = y + yMove\n",
    "        \n",
    "           \n",
    "        wallFlag = 0\n",
    "        collisionFlag = 0\n",
    "        [x, y, wallFlag] = ckWall(x,y)\n",
    "        pygame.draw.circle(screen, [100, 100, 255], [x,y], 10, 0)\n",
    "#         if wallFlag == -1:\n",
    "#             print(\"Wall!\", action)\n",
    "        next_state = stateGenerator(obstaclePos, [x,y])\n",
    "\n",
    "        if(math.sqrt((x - goalPos[0])**2 + (y - goalPos[1])**2) <= 20):\n",
    "            print(\"Goal Reached!\")           \n",
    "            collisionFlag = 1\n",
    "            done = 1\n",
    "        for i in range(0,obsNumber):\n",
    "            if moveObstacles:\n",
    "                obstaclePos[i][0] = obstaclePos[i][0] + random.randrange(-1,2)\n",
    "                obstaclePos[i][1] = obstaclePos[i][1] + random.randrange(-1,2)\n",
    "                [obstaclePos[i][0], obstaclePos[i][1], _] = ckWall(obstaclePos[i][0], obstaclePos[i][1])\n",
    "                obstaclePos[i] = ckInit(initPosAgent ,obstaclePos[i])\n",
    "\n",
    "            pygame.draw.circle(screen, [255, 50, 50], obstaclePos[i], 10, 0)\n",
    "            if math.sqrt((x - obstaclePos[i][0])**2 + (y - obstaclePos[i][1])**2) <= 20:\n",
    "                print(\"Collision!\")\n",
    "                collisionFlag = -1\n",
    "                ObjectIndex = i\n",
    "                done = True \n",
    "#         if wallFlag == -1:\n",
    "#             done = True\n",
    "        \n",
    "        if not done:\n",
    "            reward = -0.1\n",
    "            if wallFlag == -1:\n",
    "                reward = -10\n",
    "        else:\n",
    "            if collisionFlag == 1:\n",
    "                reward = 10000\n",
    "                rList.append(1)\n",
    "            elif collisionFlag == -1:\n",
    "                reward = -10000\n",
    "                rList.append(0)\n",
    "#             next_state, reward, done, ininitPosAgentfo = env.step(action)\n",
    "#             next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "#             reward = reward if not done or score == 499 else -100\n",
    "        \n",
    "        agent.train_model(state, action, reward, next_state, done)\n",
    "        \n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode, plot the play time\n",
    "            obstaclePos = [[0, 0] for _ in range(obsNumber)]\n",
    "            for i in range(0,obsNumber):\n",
    "                while True:\n",
    "                    #임시로 더 좋은 학습을 위해 random range를 살짝 수정함.\n",
    "                    obstaclePos[i][0] = boundaryPos[0] + random.randrange(1, boundaryLength[0])\n",
    "                    obstaclePos[i][1] = boundaryPos[1] + random.randrange(1, boundaryLength[1])\n",
    "                    if obstaclePos[i][0] < goalPos[0] - agentRadius or obstaclePos[i][0] > goalPos[0] + agentRadius:\n",
    "                        if obstaclePos[i][1] > goalPos[1] + agentRadius or obstaclePos[i][1] < goalPos[1] - agentRadius:\n",
    "                            if obstaclePos[i][0] < initPosAgent[0] - agentRadius - 1 or obstaclePos[i][0] > initPosAgent[0] + agentRadius + 1:\n",
    "                                if obstaclePos[i][1] > initPosAgent[1] + agentRadius + 1 or obstaclePos[i][1] < initPosAgent[1] - agentRadius - 1:\n",
    "                                    break\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rList, 'b')\n",
    "            \n",
    "        #circle(Surface, color, pos, radius, width=0)\n",
    "        pygame.draw.circle(screen, [100,255,100], [goalPos[0],goalPos[1]], 10, 2)\n",
    "        #rect(Surface, color, Rect, width=0)\n",
    "        pygame.draw.rect(screen, [255,100,100],[boundaryPos[0] - agentRadius, boundaryPos[1] - agentRadius, boundaryLength[0] + agentRadius * 2, boundaryLength[1] + agentRadius * 2],2)\n",
    "        pygame.display.flip()\n",
    "        screen.fill([200,200,200])\n",
    "    print score\n",
    "    # save the model\n",
    "    if e % 50 == 0:\n",
    "        agent.actor.save_weights(\"./Practice004_DataSave/Actor.h5\")\n",
    "        agent.critic.save_weights(\"./Practice004_DataSave/Critic.h5\")\n",
    "#         pylab.savefig(\"./Practice004_DataSave/ActorCriticGraph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 43.4752174928%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUoMMd0lqkOEuSQ36X7QJgaQ0fGHmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4634702c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Percent of successful episodes: \" + str(100.0 * sum(rList)/num_episodes) + \"%\")\n",
    "\n",
    "plt.bar(range(len(rList)), rList, color = \"Blue\", width = 0.00001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
