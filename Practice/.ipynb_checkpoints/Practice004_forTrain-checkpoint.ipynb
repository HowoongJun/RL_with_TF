{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pygame, sys, random\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "num_episodes = 50000\n",
    "\n",
    "agentRadius = 10\n",
    "\n",
    "# get size of state and action from environment\n",
    "\n",
    "boundaryPos = [400, 590]\n",
    "boundaryLength = [100,100]\n",
    "initPosAgent = [boundaryPos[0] + boundaryLength[0] / 2, boundaryPos[1] + boundaryLength[1] / 2]\n",
    "goalPos = [boundaryPos[0] + boundaryLength[0], boundaryPos[1]]\n",
    "\n",
    "moveObstacles = True\n",
    "action_size = 8\n",
    "obsNumber = 1\n",
    "state_size = obsNumber * 2 + 1\n",
    "# state_size = obsNumber + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A2C(Advantage Actor-Critic) agent\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.load_model = True\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.00002\n",
    "        self.critic_lr = 0.00005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"./Practice004_DataSave/Actor.h5\")\n",
    "            self.critic.load_weights(\"./Practice004_DataSave/Critic.h5\")\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax', kernel_initializer='glorot_normal'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        critic.add(Dense(self.value_size, activation='linear', kernel_initializer='glorot_normal'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "#         print policy\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ckWall(xPos, yPos):\n",
    "    flagWall = 0\n",
    "    if(xPos < boundaryPos[0]):\n",
    "        xPos = boundaryPos[0]\n",
    "        flagWall = -1\n",
    "    elif(xPos > boundaryPos[0] + boundaryLength[0]):\n",
    "        xPos = boundaryPos[0] + boundaryLength[0]\n",
    "        flagWall = -1\n",
    "    if(yPos < boundaryPos[1]):\n",
    "        yPos = boundaryPos[1]\n",
    "        flagWall = -1\n",
    "    elif(yPos > boundaryPos[1] + boundaryLength[1]):\n",
    "        yPos = boundaryPos[1] + boundaryLength[1]\n",
    "        flagWall = -1\n",
    "        \n",
    "    return [xPos, yPos, flagWall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ckInit(initLoc, obsLoc):\n",
    "    if math.sqrt((initLoc[0] - obsLoc[0])**2 + (initLoc[1] - obsLoc[1])**2) <= 2*agentRadius + 1:\n",
    "        if initLoc[0] - obsLoc[0] < 0:\n",
    "            obsLoc[0] += 1\n",
    "        else:\n",
    "            obsLoc[0] -= 1\n",
    "        if initLoc[1] - obsLoc[1] < 0:\n",
    "            obsLoc[1] += 1\n",
    "        else:\n",
    "            obsLoc[1] -= 1\n",
    "    if math.sqrt((goalPos[0] - obsLoc[0])**2 + (goalPos[1] - obsLoc[1])**2) <= 2*agentRadius + 1:\n",
    "        obsLoc[0] -= 1\n",
    "        obsLoc[1] += 1\n",
    "    return obsLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stateGenerator(obsPosition, agtPosition):\n",
    "    returnSum = []\n",
    "    for i in range(0,obsNumber):\n",
    "#         returnSum = returnSum + [math.sqrt((agtPosition[0] - obsPosition[i][0])**2 + (agtPosition[1] - obsPosition[i][1])**2)]\n",
    "        returnSum = returnSum + [agtPosition[0] - obsPosition[i][0], agtPosition[1] - obsPosition[i][1]]\n",
    "#     returnSum = returnSum + [agtPosition[0] - 640, agtPosition[1] - 450]\n",
    "    returnSum = returnSum + [math.sqrt((agtPosition[0] - goalPos[0])**2 + (agtPosition[1] - goalPos[1])**2)]\n",
    "    returnSum = np.reshape(returnSum, [1, state_size])\n",
    "    return returnSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def takeAction(action):\n",
    "    xAction = 0\n",
    "    yAction = 0\n",
    "    if action == 0:\n",
    "        xAction = 1\n",
    "    elif action == 1:\n",
    "        xAction = 1\n",
    "        yAction = 1\n",
    "    elif action == 2:\n",
    "        xAction = 1\n",
    "        yAction = -1            \n",
    "    elif action == 3:\n",
    "        xAction = -1\n",
    "        yAction = 1\n",
    "    elif action == 4:\n",
    "        xAction = -1\n",
    "    elif action == 5:\n",
    "        xAction = -1\n",
    "        yAction = -1\n",
    "    elif action == 6:\n",
    "        yAction = -1\n",
    "    elif action == 7:\n",
    "        yAction = 1\n",
    "        \n",
    "    return [xAction, yAction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 1,544\n",
      "Trainable params: 1,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 641\n",
      "Trainable params: 641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Episode ', 0, 'Starts!')\n",
      "Goal Reached!\n",
      "9998.9\n",
      "('Episode ', 1, 'Starts!')\n",
      "Goal Reached!\n",
      "9998.9\n",
      "('Episode ', 2, 'Starts!')\n",
      "Goal Reached!\n",
      "9998.9\n",
      "('Episode ', 3, 'Starts!')\n",
      "Goal Reached!\n",
      "9998.9\n",
      "('Episode ', 4, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 5, 'Starts!')\n",
      "Goal Reached!\n",
      "9998.9\n",
      "('Episode ', 6, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 7, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 8, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 9, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 10, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 11, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 12, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 13, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 14, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 15, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 16, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 17, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 18, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 19, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 20, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 21, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 22, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 23, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 24, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 25, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 26, 'Starts!')\n",
      "Goal Reached!\n",
      "9998.9\n",
      "('Episode ', 27, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 28, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 29, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 30, 'Starts!')\n",
      "Goal Reached!\n",
      "9998.9\n",
      "('Episode ', 31, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 32, 'Starts!')\n",
      "Goal Reached!\n",
      "9998.9\n",
      "('Episode ', 33, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 34, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 35, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 36, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 37, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 38, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 39, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 40, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 41, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 42, 'Starts!')\n",
      "Goal Reached!\n",
      "9999.0\n",
      "('Episode ', 43, 'Starts!')\n"
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "screen = pygame.display.set_mode([1280,960])\n",
    "screen.fill([200, 200, 200])\n",
    "\n",
    "# make A2C agent\n",
    "agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "rList, episodes = [], []\n",
    "\n",
    "# Make Obstacles (obsNumber)\n",
    "obstaclePos = [[0, 0] for _ in range(obsNumber)]\n",
    "for i in range(0,obsNumber):\n",
    "    while True:\n",
    "        obstaclePos[i][0] = boundaryPos[0] + random.randrange(1, boundaryLength[0])\n",
    "        obstaclePos[i][1] = boundaryPos[1] + random.randrange(1, boundaryLength[1])\n",
    "        if obstaclePos[i][0] <= goalPos[0] - agentRadius or obstaclePos[i][0] >= goalPos[0] + agentRadius:\n",
    "            if obstaclePos[i][1] >= goalPos[1] + agentRadius or obstaclePos[i][1] <= goalPos[1] - agentRadius:\n",
    "                if obstaclePos[i][0] <= initPosAgent[0] - agentRadius or obstaclePos[i][0] >= initPosAgent[0] + agentRadius:\n",
    "                    if obstaclePos[i][1] >= initPosAgent[1] + agentRadius or obstaclePos[i][1] <= initPosAgent[1] - agentRadius:\n",
    "                        break\n",
    "                \n",
    "for e in range(num_episodes):\n",
    "    # Initialize\n",
    "    done = False\n",
    "    score = 0\n",
    "    x = initPosAgent[0]#boundaryPos[0]\n",
    "    y = initPosAgent[1]#boundaryPos[1]\n",
    "    print(\"Episode \", e, \"Starts!\")\n",
    "    state = stateGenerator(obstaclePos, [x,y])\n",
    "    #state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        xMove = 0\n",
    "        yMove = 0\n",
    "        \n",
    "        [xMove, yMove] = takeAction(action)\n",
    "        \n",
    "        x = x + xMove\n",
    "        y = y + yMove\n",
    "\n",
    "        wallFlag = 0\n",
    "        collisionFlag = 0\n",
    "        [x, y, wallFlag] = ckWall(x,y)\n",
    "        pygame.draw.circle(screen, [100, 100, 255], [x,y], 10, 0)\n",
    "#         if wallFlag == -1:\n",
    "#             print(\"Wall!\", action)\n",
    "        next_state = stateGenerator(obstaclePos, [x,y])\n",
    "\n",
    "        if(math.sqrt((x - goalPos[0])**2 + (y - goalPos[1])**2) <= 20):\n",
    "            print(\"Goal Reached!\")           \n",
    "            collisionFlag = 1\n",
    "            done = 1\n",
    "        for i in range(0,obsNumber):\n",
    "            if moveObstacles:\n",
    "                obstaclePos[i][0] = obstaclePos[i][0] + random.randrange(-1,2)\n",
    "                obstaclePos[i][1] = obstaclePos[i][1] + random.randrange(-1,2)\n",
    "                [obstaclePos[i][0], obstaclePos[i][1], _] = ckWall(obstaclePos[i][0], obstaclePos[i][1])\n",
    "                obstaclePos[i] = ckInit(initPosAgent ,obstaclePos[i])\n",
    "\n",
    "            pygame.draw.circle(screen, [255, 50, 50], obstaclePos[i], 10, 0)\n",
    "            if math.sqrt((x - obstaclePos[i][0])**2 + (y - obstaclePos[i][1])**2) <= 20:\n",
    "                print(\"Collision!\")\n",
    "                collisionFlag = -1\n",
    "                ObjectIndex = i\n",
    "                done = True \n",
    "#         if wallFlag == -1:\n",
    "#             done = True\n",
    "            \n",
    "        if not done:\n",
    "            reward = -0.1\n",
    "            if wallFlag == -1:\n",
    "                reward = -1\n",
    "        else:\n",
    "            if collisionFlag == 1:\n",
    "                reward = 10000\n",
    "                rList.append(1)\n",
    "            elif collisionFlag == -1:\n",
    "                reward = -10000\n",
    "                rList.append(0)\n",
    "#             next_state, reward, done, ininitPosAgentfo = env.step(action)\n",
    "#             next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "#             reward = reward if not done or score == 499 else -100\n",
    "        \n",
    "        agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode, plot the play time\n",
    "\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rList, 'b')\n",
    "            pylab.savefig(\"./Practice004_DataSave/ActorCriticGraph.png\")\n",
    "        #circle(Surface, color, pos, radius, width=0)\n",
    "        pygame.draw.circle(screen, [100,255,100], [goalPos[0],goalPos[1]], 10, 2)\n",
    "        #rect(Surface, color, Rect, width=0)\n",
    "        pygame.draw.rect(screen, [255,100,100],[boundaryPos[0] - agentRadius, boundaryPos[1] - agentRadius, boundaryLength[0] + agentRadius * 2, boundaryLength[1] + agentRadius * 2],2)\n",
    "        pygame.display.flip()\n",
    "        screen.fill([200,200,200])\n",
    "    print score\n",
    "    # save the model\n",
    "    if e % 50 == 0:\n",
    "        agent.actor.save_weights(\"./Practice004_DataSave/Actor.h5\")\n",
    "        agent.critic.save_weights(\"./Practice004_DataSave/Critic.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 23.322%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of successful episodes: \" + str(100.0 * sum(rList)/num_episodes) + \"%\")\n",
    "\n",
    "plt.bar(range(len(rList)), rList, color = \"Blue\", width = 0.00001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
