{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame, sys, random\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from fractions import Fraction\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "obstacleRadius = 10\n",
    "agentRadius = 10\n",
    "\n",
    "# get size of state and action from environment\n",
    "\n",
    "boundaryPos = [100, 100]\n",
    "boundaryLength = [70,70]\n",
    "boundaryRadius = 40\n",
    "dispSize = [1280, 960]\n",
    "initPosAgentStandard = [dispSize[0] - 100, 100]#dispSize[1]]\n",
    "initPosAgent = initPosAgentStandard#[boundaryPos[0] + boundaryLength[0] / 2, boundaryPos[1] + boundaryLength[1] / 2]\n",
    "goalPos = [100, 900]\n",
    "goalAngle = 0#random.randrange(0, 360) * math.pi / 180\n",
    "\n",
    "obstacleRandomRange = 1000\n",
    "\n",
    "moveObstacles = True\n",
    "action_size = 9\n",
    "obsNumber = 100\n",
    "state_size = 2\n",
    "# state_size = obsNumber + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A2C(Advantage Actor-Critic) agent\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.load_model = True\n",
    "        \n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.00002\n",
    "        self.critic_lr = 0.00005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        if self.load_model:\n",
    "#             self.actor.load_weights(\"./Practice004_DataSave/Backup/Actor_Rev_171221_1000.h5\")\n",
    "#             self.critic.load_weights(\"./Practice004_DataSave/Backup/Critic_Rev_171221_1000.h5\")\n",
    "            self.actor.load_weights(\"./Practice004_DataSave/Actor_Rev.h5\")\n",
    "            self.critic.load_weights(\"./Practice004_DataSave/Critic_Rev.h5\")\n",
    "\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax', kernel_initializer='glorot_normal'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        critic.add(Dense(self.value_size, activation='linear', kernel_initializer='glorot_normal'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "#         print policy\n",
    "        return policy\n",
    "#         return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stateGenerator(obsPosition, agtPosition, idx):\n",
    "    returnSum = []\n",
    "    if idx != -1:\n",
    "        returnSum = returnSum + [agtPosition[0] - obsPosition[idx][0], agtPosition[1] - obsPosition[idx][1]]\n",
    "    else:\n",
    "        returnSum = returnSum + [agtPosition[0] - obsPosition[0], agtPosition[1] - obsPosition[1]]\n",
    "    returnSum = np.reshape(returnSum, [1, 2])\n",
    "    return returnSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def takeAction(action):\n",
    "    xAction = 0\n",
    "    yAction = 0\n",
    "    if action == 0:\n",
    "        xAction = 1\n",
    "    elif action == 1:\n",
    "        xAction = 1\n",
    "        yAction = 1\n",
    "    elif action == 2:\n",
    "        xAction = 1\n",
    "        yAction = -1            \n",
    "    elif action == 3:\n",
    "        xAction = -1\n",
    "        yAction = 1\n",
    "    elif action == 4:\n",
    "        xAction = -1\n",
    "    elif action == 5:\n",
    "        xAction = -1\n",
    "        yAction = -1\n",
    "    elif action == 6:\n",
    "        yAction = -1\n",
    "    elif action == 7:\n",
    "        yAction = 1\n",
    "    elif action == 8:\n",
    "        xAction = 0\n",
    "        yAction = 0\n",
    "        \n",
    "    return [xAction, yAction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rangeFinder(allObsPos, rangeCenter):\n",
    "    countObs = 0\n",
    "    rangeObstacle = [[0,0] for _ in range(obsNumber)]\n",
    "    for i in range(0, obsNumber):\n",
    "        if math.sqrt((rangeCenter[0] - allObsPos[i][0])**2 + (rangeCenter[1] - allObsPos[i][1])**2) < boundaryRadius:\n",
    "            rangeObstacle[countObs] = allObsPos[i]\n",
    "            countObs += 1\n",
    "            \n",
    "    return [countObs, rangeObstacle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def goalFinder(agtPos):\n",
    "    if goalPos[0] == agtPos[0]:\n",
    "        if goalPos[1] > agtPos[1]:\n",
    "            goalAngle = 90 * math.pi / 180\n",
    "        else:\n",
    "            goalAngle = -90 * math.pi / 180\n",
    "    else:\n",
    "        goalAngle = math.atan(1.0*(goalPos[1]-agtPos[1])/(goalPos[0]-agtPos[0]))\n",
    "    if goalPos[0] < agtPos[0]:\n",
    "        goalAngle += math.pi\n",
    "        \n",
    "    tmpGoal = [0,0]\n",
    "    tmpGoal[0] = int(math.floor(agtPos[0] + boundaryRadius * math.cos(goalAngle)))\n",
    "    tmpGoal[1] = int(math.floor(agtPos[1] + boundaryRadius * math.sin(goalAngle)))\n",
    "    return tmpGoal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearestAction(actionIdx):\n",
    "    nearAction = []\n",
    "    if actionIdx == 0:\n",
    "        nearAction = [1, 2]\n",
    "    elif actionIdx == 1:\n",
    "        nearAction = [0, 7]\n",
    "    elif actionIdx == 2:\n",
    "        nearAction = [0, 6]\n",
    "    elif actionIdx == 3:\n",
    "        nearAction = [4, 7]\n",
    "    elif actionIdx == 4:\n",
    "        nearAction = [3, 5]\n",
    "    elif actionIdx == 5:\n",
    "        nearAction = [4, 6]\n",
    "    elif actionIdx == 6:\n",
    "        nearAction = [5, 2]\n",
    "    elif actionIdx == 7:\n",
    "        nearAction = [1, 3]\n",
    "    else:\n",
    "        nearAction = [8, 8]\n",
    "    return nearAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 1,545\n",
      "Trainable params: 1,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 513\n",
      "Trainable params: 513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Episode ', 0, 'Starts!')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/howoongjun/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:41: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal Reached!\n",
      "10302.1\n",
      "('Episode ', 1, 'Starts!')\n",
      "Goal Reached!\n",
      "10200.9\n",
      "('Episode ', 2, 'Starts!')\n",
      "Goal Reached!\n",
      "10117.5\n",
      "('Episode ', 3, 'Starts!')\n",
      "Goal Reached!\n",
      "10294.3\n",
      "('Episode ', 4, 'Starts!')\n",
      "Goal Reached!\n",
      "10170.6\n",
      "('Episode ', 5, 'Starts!')\n",
      "Goal Reached!\n",
      "10347.1\n",
      "('Episode ', 6, 'Starts!')\n",
      "Goal Reached!\n",
      "10154.1\n",
      "('Episode ', 7, 'Starts!')\n",
      "Goal Reached!\n",
      "10247.6\n",
      "('Episode ', 8, 'Starts!')\n",
      "Goal Reached!\n",
      "10230.5\n",
      "('Episode ', 9, 'Starts!')\n",
      "Goal Reached!\n",
      "10144.4\n",
      "('Episode ', 10, 'Starts!')\n",
      "Goal Reached!\n",
      "10185.4\n",
      "('Episode ', 11, 'Starts!')\n",
      "Goal Reached!\n",
      "10162.2\n",
      "('Episode ', 12, 'Starts!')\n",
      "Goal Reached!\n",
      "10127.4\n",
      "('Episode ', 13, 'Starts!')\n",
      "Goal Reached!\n",
      "10266.6\n",
      "('Episode ', 14, 'Starts!')\n",
      "Goal Reached!\n",
      "10163.5\n",
      "('Episode ', 15, 'Starts!')\n",
      "Goal Reached!\n",
      "10229.8\n",
      "('Episode ', 16, 'Starts!')\n",
      "Goal Reached!\n",
      "10197.9\n",
      "('Episode ', 17, 'Starts!')\n",
      "Goal Reached!\n",
      "10132.9\n",
      "('Episode ', 18, 'Starts!')\n",
      "Goal Reached!\n",
      "10358.8\n",
      "('Episode ', 19, 'Starts!')\n",
      "Collision!\n",
      "-9936.5\n",
      "('Episode ', 20, 'Starts!')\n",
      "Goal Reached!\n",
      "10212.5\n",
      "('Episode ', 21, 'Starts!')\n",
      "Goal Reached!\n",
      "10373.7\n",
      "('Episode ', 22, 'Starts!')\n",
      "Goal Reached!\n",
      "10555.6\n",
      "('Episode ', 23, 'Starts!')\n",
      "Goal Reached!\n",
      "10340.1\n",
      "('Episode ', 24, 'Starts!')\n",
      "Goal Reached!\n",
      "10134.5\n",
      "('Episode ', 25, 'Starts!')\n",
      "Goal Reached!\n",
      "10155.1\n",
      "('Episode ', 26, 'Starts!')\n",
      "Goal Reached!\n",
      "10169.6\n",
      "('Episode ', 27, 'Starts!')\n",
      "Goal Reached!\n",
      "10397.1\n",
      "('Episode ', 28, 'Starts!')\n",
      "Goal Reached!\n",
      "10257.0\n",
      "('Episode ', 29, 'Starts!')\n",
      "Collision!\n",
      "-9858.8\n",
      "('Episode ', 30, 'Starts!')\n",
      "Goal Reached!\n",
      "10106.9\n",
      "('Episode ', 31, 'Starts!')\n",
      "Goal Reached!\n",
      "10121.8\n",
      "('Episode ', 32, 'Starts!')\n",
      "Goal Reached!\n",
      "10119.9\n",
      "('Episode ', 33, 'Starts!')\n",
      "Goal Reached!\n",
      "10168.9\n",
      "('Episode ', 34, 'Starts!')\n",
      "Goal Reached!\n",
      "10146.3\n",
      "('Episode ', 35, 'Starts!')\n",
      "Goal Reached!\n",
      "10266.4\n",
      "('Episode ', 36, 'Starts!')\n",
      "Goal Reached!\n",
      "10331.9\n",
      "('Episode ', 37, 'Starts!')\n",
      "Goal Reached!\n",
      "10190.0\n",
      "('Episode ', 38, 'Starts!')\n",
      "Goal Reached!\n",
      "10170.7\n",
      "('Episode ', 39, 'Starts!')\n",
      "Goal Reached!\n",
      "10119.4\n",
      "('Episode ', 40, 'Starts!')\n",
      "Goal Reached!\n",
      "10108.9\n",
      "('Episode ', 41, 'Starts!')\n",
      "Goal Reached!\n",
      "10119.6\n",
      "('Episode ', 42, 'Starts!')\n",
      "Goal Reached!\n",
      "10246.8\n",
      "('Episode ', 43, 'Starts!')\n",
      "Goal Reached!\n",
      "10636.0\n",
      "('Episode ', 44, 'Starts!')\n",
      "Collision!\n",
      "-9927.3\n",
      "('Episode ', 45, 'Starts!')\n",
      "Goal Reached!\n",
      "10191.7\n",
      "('Episode ', 46, 'Starts!')\n",
      "Goal Reached!\n",
      "10232.7\n",
      "('Episode ', 47, 'Starts!')\n",
      "Goal Reached!\n",
      "10327.2\n",
      "('Episode ', 48, 'Starts!')\n",
      "Goal Reached!\n",
      "10122.8\n",
      "('Episode ', 49, 'Starts!')\n",
      "Collision!\n",
      "-9880.2\n",
      "('Episode ', 50, 'Starts!')\n",
      "Goal Reached!\n",
      "10177.9\n",
      "('Episode ', 51, 'Starts!')\n",
      "Goal Reached!\n",
      "10229.8\n",
      "('Episode ', 52, 'Starts!')\n",
      "Goal Reached!\n",
      "10296.4\n",
      "('Episode ', 53, 'Starts!')\n",
      "Goal Reached!\n",
      "10118.9\n",
      "('Episode ', 54, 'Starts!')\n",
      "Goal Reached!\n",
      "10120.6\n",
      "('Episode ', 55, 'Starts!')\n",
      "Goal Reached!\n",
      "10189.6\n",
      "('Episode ', 56, 'Starts!')\n",
      "Goal Reached!\n",
      "10122.0\n",
      "('Episode ', 57, 'Starts!')\n",
      "Goal Reached!\n",
      "10308.5\n",
      "('Episode ', 58, 'Starts!')\n",
      "Goal Reached!\n",
      "10166.8\n",
      "('Episode ', 59, 'Starts!')\n",
      "Collision!\n",
      "-9841.2\n",
      "('Episode ', 60, 'Starts!')\n",
      "Goal Reached!\n",
      "10176.0\n",
      "('Episode ', 61, 'Starts!')\n",
      "Goal Reached!\n",
      "10264.5\n",
      "('Episode ', 62, 'Starts!')\n",
      "Goal Reached!\n",
      "10432.4\n",
      "('Episode ', 63, 'Starts!')\n",
      "Goal Reached!\n",
      "10191.0\n",
      "('Episode ', 64, 'Starts!')\n",
      "Goal Reached!\n",
      "10115.6\n",
      "('Episode ', 65, 'Starts!')\n",
      "Goal Reached!\n",
      "10581.7\n",
      "('Episode ', 66, 'Starts!')\n",
      "Goal Reached!\n",
      "10333.5\n",
      "('Episode ', 67, 'Starts!')\n",
      "Goal Reached!\n",
      "10132.4\n",
      "('Episode ', 68, 'Starts!')\n",
      "Collision!\n",
      "-9899.5\n",
      "('Episode ', 69, 'Starts!')\n",
      "Goal Reached!\n",
      "10130.6\n",
      "('Episode ', 70, 'Starts!')\n",
      "Goal Reached!\n",
      "10108.1\n",
      "('Episode ', 71, 'Starts!')\n",
      "Goal Reached!\n",
      "10232.5\n",
      "('Episode ', 72, 'Starts!')\n",
      "Goal Reached!\n",
      "10219.9\n",
      "('Episode ', 73, 'Starts!')\n",
      "Goal Reached!\n",
      "10171.3\n",
      "('Episode ', 74, 'Starts!')\n",
      "Goal Reached!\n",
      "10323.9\n",
      "('Episode ', 75, 'Starts!')\n",
      "Goal Reached!\n",
      "10110.2\n",
      "('Episode ', 76, 'Starts!')\n",
      "Goal Reached!\n",
      "10209.9\n",
      "('Episode ', 77, 'Starts!')\n",
      "Goal Reached!\n",
      "10214.0\n",
      "('Episode ', 78, 'Starts!')\n",
      "Goal Reached!\n",
      "10335.2\n",
      "('Episode ', 79, 'Starts!')\n",
      "Goal Reached!\n",
      "10106.9\n",
      "('Episode ', 80, 'Starts!')\n",
      "Goal Reached!\n",
      "10129.6\n",
      "('Episode ', 81, 'Starts!')\n",
      "Goal Reached!\n",
      "10132.2\n",
      "('Episode ', 82, 'Starts!')\n",
      "Goal Reached!\n",
      "10117.6\n",
      "('Episode ', 83, 'Starts!')\n",
      "Goal Reached!\n",
      "10127.6\n",
      "('Episode ', 84, 'Starts!')\n",
      "Goal Reached!\n",
      "10226.7\n",
      "('Episode ', 85, 'Starts!')\n",
      "Goal Reached!\n",
      "10488.9\n",
      "('Episode ', 86, 'Starts!')\n",
      "Goal Reached!\n",
      "10608.0\n",
      "('Episode ', 87, 'Starts!')\n",
      "Goal Reached!\n",
      "10113.7\n",
      "('Episode ', 88, 'Starts!')\n",
      "Goal Reached!\n",
      "10658.0\n",
      "('Episode ', 89, 'Starts!')\n",
      "Collision!\n",
      "-9977.0\n",
      "('Episode ', 90, 'Starts!')\n",
      "Goal Reached!\n",
      "10271.1\n",
      "('Episode ', 91, 'Starts!')\n",
      "Goal Reached!\n",
      "10116.1\n",
      "('Episode ', 92, 'Starts!')\n",
      "Goal Reached!\n",
      "10142.6\n",
      "('Episode ', 93, 'Starts!')\n",
      "Goal Reached!\n",
      "10283.6\n",
      "('Episode ', 94, 'Starts!')\n",
      "Goal Reached!\n",
      "10296.4\n",
      "('Episode ', 95, 'Starts!')\n",
      "Goal Reached!\n",
      "10212.8\n",
      "('Episode ', 96, 'Starts!')\n",
      "Goal Reached!\n",
      "10401.4\n",
      "('Episode ', 97, 'Starts!')\n",
      "Goal Reached!\n",
      "10167.1\n",
      "('Episode ', 98, 'Starts!')\n",
      "Goal Reached!\n",
      "10275.1\n",
      "('Episode ', 99, 'Starts!')\n",
      "Goal Reached!\n",
      "10174.7\n"
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "screen = pygame.display.set_mode(dispSize)\n",
    "screen.fill([200, 200, 200])\n",
    "\n",
    "# make A2C agent\n",
    "agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "rList, episodes = [], []\n",
    "\n",
    "# Make Obstacles (obsNumber)\n",
    "obstaclePos = [[0, 0] for _ in range(obsNumber)]\n",
    "for i in range(0,obsNumber):\n",
    "#     obsRadius = random.randrange(agentRadius + obstacleRadius + 10, obstacleRandomRange)\n",
    "#     obsAngle = random.randrange(90,180) * math.pi / 180\n",
    "#     obstaclePos[i][0] = int(initPosAgent[0] + obsRadius * math.cos(obsAngle)) #boundaryPos[0] + random.randrange(1, boundaryLength[0])\n",
    "#     obstaclePos[i][1] = int(initPosAgent[1] + obsRadius * math.sin(obsAngle)) #boundaryPos[1] + random.randrange(1, boundaryLength[1])\n",
    "    obstaclePos[i][0] = int(initPosAgent[0] - (obstacleRadius + agentRadius + random.randrange(0, dispSize[0])))\n",
    "    obstaclePos[i][1] = int(initPosAgent[1] + (obstacleRadius + agentRadius + random.randrange(0, dispSize[1])))\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    # Initialize\n",
    "    done = False\n",
    "    score = 0\n",
    "    x = initPosAgent[0]\n",
    "    y = initPosAgent[1]\n",
    "    print(\"Episode \", e, \"Starts!\")\n",
    "    \n",
    "    while not done:\n",
    "        [rangeObsNumber, rangeObsPos] = rangeFinder(obstaclePos, initPosAgent)\n",
    "#         print rangeObsNumber\n",
    "        tmpAction = []\n",
    "        for i in range(0,rangeObsNumber):\n",
    "            state = stateGenerator(rangeObsPos, [x,y], i)\n",
    "            policyArr = agent.get_action(state)\n",
    "            if i == 0:\n",
    "                tmpAction = (1 - policyArr)\n",
    "            else:\n",
    "                tmpAction = tmpAction * (1 - policyArr)\n",
    "#         print(\"0: \", tmpAction)\n",
    "       \n",
    "        if tmpAction != []:\n",
    "            for j in range(0, 9):\n",
    "                if tmpAction[j] > 0.9999:\n",
    "                    tmpAction[j] = 1\n",
    "                else:\n",
    "                    tmpAction[j] = 0\n",
    "            tmpArgMax = np.argmax(tmpAction)\n",
    "            \n",
    "#         tmpAction = [round(elem,0) for elem in tmpAction]\n",
    "\n",
    "        if rangeObsNumber == 0:\n",
    "            tmpAction = [1.0/9.0 for _ in range(0, 9)]\n",
    "        \n",
    "        tmpGoalPos = goalFinder([x, y])\n",
    "        state = stateGenerator(tmpGoalPos, [x,y], -1)\n",
    "        policyArr = agent.get_action(state)\n",
    "#         nearAction = nearestAction(np.argmax(policyArr))\n",
    "#         policyArr[nearAction[0]] = max(policyArr)/3\n",
    "#         policyArr[nearAction[1]] = max(policyArr)/3\n",
    "#         print(\"1: \", tmpAction)\n",
    "        if np.mean(tmpAction) == 0:\n",
    "            tmpAction[tmpArgMax] = 1\n",
    "\n",
    "#         for i in range(0,9):\n",
    "#             if policyArr[i] == max(policyArr):\n",
    "#                 break\n",
    "#         policyArr = [1.0 for _ in range(0,9)]\n",
    "#         policyArr[i] = 10000.0\n",
    "        tmpAction = tmpAction * np.asarray(policyArr)\n",
    "        \n",
    "        tmpAction = tmpAction / np.sum(tmpAction)\n",
    "#         print(\"2: \", policyArr)\n",
    "#         print(\"3: \", tmpAction)\n",
    "#         print(\"==============================================================================================================\")\n",
    "        action = np.random.choice(action_size, 1, p = tmpAction)[0]\n",
    "#         if tmpAction[np.argmax(policyArr)] == 0:\n",
    "#             while True:\n",
    "#                 action = random.randrange(0,9)\n",
    "#                 if tmpAction[action] != 0:\n",
    "#                     break\n",
    "                    \n",
    "        xMove = 0\n",
    "        yMove = 0\n",
    "\n",
    "        [xMove, yMove] = takeAction(action)\n",
    "\n",
    "        x = x + xMove\n",
    "        y = y + yMove\n",
    "\n",
    "        wallFlag = 0\n",
    "        collisionFlag = 0\n",
    "#         [x, y] = ckWall(x, y)\n",
    "        pygame.draw.circle(screen, [100, 100, 255], [x,y], 10, 0)\n",
    "#         next_state = stateGenerator(obstaclePos, [x,y])\n",
    "        initPosAgent = [x,y]\n",
    "\n",
    "        if math.sqrt((x -  goalPos[0])**2 + (y - goalPos[1])**2) <= agentRadius:\n",
    "            print(\"Goal Reached!\")\n",
    "            collisionFlag = 1\n",
    "            done = True\n",
    "        for i in range(0,obsNumber):\n",
    "            if moveObstacles:\n",
    "                obstaclePos[i][0] = obstaclePos[i][0] + random.randrange(-1,2)\n",
    "                obstaclePos[i][1] = obstaclePos[i][1] + random.randrange(-1,2)\n",
    "                \n",
    "            pygame.draw.circle(screen, [255, 50, 50], obstaclePos[i], obstacleRadius, 0)\n",
    "            if math.sqrt((x - obstaclePos[i][0])**2 + (y - obstaclePos[i][1])**2) < 19:\n",
    "                print(\"Collision!\")\n",
    "                collisionFlag = -1\n",
    "                done = True\n",
    "#                 break\n",
    "        \n",
    "        if not done:\n",
    "            reward = 0.1\n",
    "\n",
    "        else:\n",
    "            if collisionFlag == 1:\n",
    "                reward = 10000 * math.cos(math.atan2(y - initPosAgent[1], x - initPosAgent[0]))\n",
    "                rList.append(1)\n",
    "            elif collisionFlag == -1:\n",
    "                reward = -10000\n",
    "                rList.append(0)\n",
    "        \n",
    "#         agent.train_model(state, action, reward, next_state, done)\n",
    "        \n",
    "        score += reward\n",
    "#         state = next_state\n",
    "#         if score >= 10000:\n",
    "#             print \"Success!\"\n",
    "#             done = True\n",
    "        pygame.draw.circle(screen, [255,100,100], initPosAgent, boundaryRadius, 2)\n",
    "\n",
    "        if done:\n",
    "            # every episode, plot the play time\n",
    "            initPosAgent = initPosAgentStandard\n",
    "            obstaclePos = [[0, 0] for _ in range(obsNumber)]\n",
    "            for i in range(0,obsNumber):\n",
    "#                 obsRadius = random.randrange(agentRadius + obstacleRadius + 100, obstacleRandomRange)\n",
    "#                 obsAngle = random.randrange(90,180) * math.pi / 180\n",
    "#                 obstaclePos[i][0] = int(initPosAgent[0] + obsRadius * math.cos(obsAngle))\n",
    "#                 obstaclePos[i][1] = int(initPosAgent[1] + obsRadius * math.sin(obsAngle))\n",
    "                obstaclePos[i][0] = int(initPosAgent[0] - (obstacleRadius + agentRadius + random.randrange(0, dispSize[0])))\n",
    "                obstaclePos[i][1] = int(initPosAgent[1] + (obstacleRadius + agentRadius + random.randrange(0, dispSize[1])))\n",
    "\n",
    "            episodes.append(e)\n",
    "            \n",
    "        #circle(Surface, color, pos, radius, width=0)\n",
    "        pygame.draw.circle(screen, [100,255,100], goalPos, 10, 2)\n",
    "        pygame.draw.circle(screen, [0, 255, 200], tmpGoalPos, 5, 5)\n",
    "        pygame.display.flip()\n",
    "        screen.fill([220,220,220])\n",
    "    print score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 93.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of successful episodes: \" + str(100.0 * sum(rList)/num_episodes) + \"%\")\n",
    "\n",
    "# plt.bar(range(len(rList)), rList, color = \"Blue\", width = 0.00001)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
