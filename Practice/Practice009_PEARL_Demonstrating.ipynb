{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pygame, sys, random\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from fractions import Fraction\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "obstacleRadius = 10\n",
    "agentRadius = 10\n",
    "\n",
    "# get size of state and action from environment\n",
    "\n",
    "boundaryPos = [100, 100]\n",
    "boundaryLength = [70,70]\n",
    "boundaryRadius = 40\n",
    "dispSize = [1280, 960]\n",
    "initPosAgentStandard = [dispSize[0] - 100, 100]#dispSize[1]]\n",
    "initPosAgent = initPosAgentStandard#[boundaryPos[0] + boundaryLength[0] / 2, boundaryPos[1] + boundaryLength[1] / 2]\n",
    "goalPos = [100, 900]\n",
    "goalAngle = 0#random.randrange(0, 360) * math.pi / 180\n",
    "\n",
    "obstacleRandomRange = 1000\n",
    "\n",
    "moveObstacles = True\n",
    "action_size = 8\n",
    "obsNumber = 100\n",
    "state_size = 2\n",
    "# state_size = obsNumber + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A2C(Advantage Actor-Critic) agent\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.load_model1 = True\n",
    "        self.load_model2 = False\n",
    "        \n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.00002\n",
    "        self.critic_lr = 0.00005\n",
    "\n",
    "        # create model for policy network\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        if self.load_model1:\n",
    "            self.actor.load_weights(\"./Practice004_DataSave/Actor_PEARL.h5\")\n",
    "            self.critic.load_weights(\"./Practice004_DataSave/Critic_PEARL.h5\")\n",
    "\n",
    "#             print self.actor.weights\n",
    "#             print self.critic.weights\n",
    "    # approximate policy and value using Neural Network\n",
    "    # actor: state is input and probability of each action is output of model\n",
    "    def build_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(Dense(1, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax', kernel_initializer='glorot_normal'))\n",
    "        actor.summary()\n",
    "        # See note regarding crossentropy in cartpole_reinforce.py\n",
    "        actor.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.actor_lr))\n",
    "        return actor\n",
    "\n",
    "    # critic: state is input and value of state is output of model\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(1, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        critic.add(Dense(self.value_size, activation='linear', kernel_initializer='glorot_normal'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "        return critic\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "#         print policy\n",
    "        return policy\n",
    "#         return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
    "        self.critic.fit(state, target, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stateGenerator(intenPref, distPref):\n",
    "    returnSum = [intenPref, distPref]\n",
    "    returnSum = np.reshape(returnSum, [1, state_size])\n",
    "#     print returnSum\n",
    "    return returnSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def takeAction(action):\n",
    "    xAction = 0\n",
    "    yAction = 0\n",
    "    if action == 0:\n",
    "        xAction = 1\n",
    "    elif action == 1:\n",
    "        xAction = 1\n",
    "        yAction = 1\n",
    "    elif action == 2:\n",
    "        xAction = 1\n",
    "        yAction = -1            \n",
    "    elif action == 3:\n",
    "        xAction = -1\n",
    "        yAction = 1\n",
    "    elif action == 4:\n",
    "        xAction = -1\n",
    "    elif action == 5:\n",
    "        xAction = -1\n",
    "        yAction = -1\n",
    "    elif action == 6:\n",
    "        yAction = -1\n",
    "    elif action == 7:\n",
    "        yAction = 1\n",
    "#     elif action == 8:\n",
    "#         xAction = 0\n",
    "#         yAction = 0\n",
    "        \n",
    "    return [xAction, yAction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeature(agtPos, goalPos, obsPos, const):\n",
    "    feature1 = (agtPos[0] - goalPos[0])**2 + (agtPos[1] - goalPos[1])**2\n",
    "    feature2 = 1 / ((agtPos[0] - obsPos[0])**2 + (agtPos[1] - obsPos[1])**2 + const)\n",
    "    return [feature1, feature2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rangeFinder(allObsPos, rangeCenter):\n",
    "    allObsAgtDistance = [0 for _ in range(obsNumber)]\n",
    "    for i in range(0, obsNumber):\n",
    "        allObsAgtDistance[i] = math.sqrt((allObsPos[i][0] - rangeCenter[0])**2 + (allObsPos[i][1] - rangeCenter[1])**2)\n",
    "    index = np.argmin(allObsAgtDistance)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 16        \n",
      "=================================================================\n",
      "Total params: 19\n",
      "Trainable params: 19\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Episode ', 0, 'Starts!')\n"
     ]
    }
   ],
   "source": [
    "pygame.init()\n",
    "screen = pygame.display.set_mode(dispSize)\n",
    "screen.fill([200, 200, 200])\n",
    "\n",
    "# make A2C agent\n",
    "agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "rList, episodes = [], []\n",
    "\n",
    "# Make Obstacles (obsNumber)\n",
    "obstaclePos = [[0, 0] for _ in range(obsNumber)]\n",
    "for i in range(0,obsNumber):\n",
    "    obstaclePos[i][0] = int(initPosAgent[0] - (obstacleRadius + agentRadius + random.randrange(0, dispSize[0])))\n",
    "    obstaclePos[i][1] = int(initPosAgent[1] + (obstacleRadius + agentRadius + random.randrange(0, dispSize[1])))\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    # Initialize\n",
    "    done = False\n",
    "    score = 0\n",
    "    x = initPosAgent[0]\n",
    "    y = initPosAgent[1]\n",
    "    print(\"Episode \", e, \"Starts!\")\n",
    "    xMove = 0\n",
    "    yMove = 0\n",
    "    while not done:\n",
    "        tmpAction = []\n",
    "        idx = rangeFinder(obstaclePos, [x, y])\n",
    "        FeatureVec1 = (x - goalPos[0])**2 + (y - goalPos[1])**2\n",
    "        FeatureVec2 = 1 / (0.2 + (x - obstaclePos[idx][0])**2 + (y - obstaclePos[idx][1])**2)\n",
    "        state = stateGenerator(FeatureVec1, FeatureVec2)\n",
    "        policyArr = agent.get_action(state)\n",
    "\n",
    "        action = np.random.choice(action_size, 1, p = policyArr)[0]\n",
    "\n",
    "        [xMove, yMove] = takeAction(action)\n",
    "\n",
    "        x = x + xMove\n",
    "        y = y + yMove\n",
    "        \n",
    "     \n",
    "        wallFlag = 0\n",
    "        collisionFlag = 0\n",
    "        pygame.draw.circle(screen, [100, 100, 255], [x,y], 10, 0)\n",
    "\n",
    "        initPosAgent = [x,y]\n",
    "   \n",
    "        if math.sqrt((x -  goalPos[0])**2 + (y - goalPos[1])**2) <= agentRadius:\n",
    "            print(\"Goal Reached!\")\n",
    "            collisionFlag = 1\n",
    "            done = True\n",
    "        for i in range(0,obsNumber):\n",
    "            if moveObstacles:\n",
    "                obstaclePos[i][0] = obstaclePos[i][0] + random.randrange(-1,2)\n",
    "                obstaclePos[i][1] = obstaclePos[i][1] + random.randrange(-1,2)\n",
    "                \n",
    "            pygame.draw.circle(screen, [255, 50, 50], obstaclePos[i], obstacleRadius, 0)\n",
    "            if math.sqrt((x - obstaclePos[i][0])**2 + (y - obstaclePos[i][1])**2) < 19:\n",
    "                print(\"Collision!\")\n",
    "                collisionFlag = -1\n",
    "                done = True\n",
    "        \n",
    "        pygame.draw.circle(screen, [255,100,100], initPosAgent, boundaryRadius, 2)\n",
    "\n",
    "        if done:\n",
    "            # every episode, plot the play time\n",
    "            initPosAgent = initPosAgentStandard\n",
    "            obstaclePos = [[0, 0] for _ in range(obsNumber)]\n",
    "            for i in range(0,obsNumber):\n",
    "                obstaclePos[i][0] = int(initPosAgent[0] - (obstacleRadius + agentRadius + random.randrange(0, dispSize[0])))\n",
    "                obstaclePos[i][1] = int(initPosAgent[1] + (obstacleRadius + agentRadius + random.randrange(0, dispSize[1])))\n",
    "\n",
    "            episodes.append(e)\n",
    "            \n",
    "        pygame.draw.circle(screen, [100,255,100], goalPos, 10, 2)\n",
    "        pygame.display.flip()\n",
    "        screen.fill([220,220,220])\n",
    "    print score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 40.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of successful episodes: \" + str(100.0 * sum(rList)/num_episodes) + \"%\")\n",
    "\n",
    "# plt.bar(range(len(rList)), rList, color = \"Blue\", width = 0.00001)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
