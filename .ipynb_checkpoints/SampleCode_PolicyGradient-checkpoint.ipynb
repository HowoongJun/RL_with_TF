{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "H = 10\n",
    "learning_rate = 2e-3\n",
    "gamma = 0.99\n",
    "decay_rate = 0.99\n",
    "score_queue_size = 100\n",
    "resume = False\n",
    "D = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if resume:  model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D)\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() }\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    return I[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "        \n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h = sigmoid(h)\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_backward(eph, epdlogp, epx):\n",
    "    global grad_buffer\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    eph_dot = eph*(1-eph)\n",
    "    dW1 = dh * eph_dot\n",
    "    dW1 = np.dot(dW1.T, epx)\n",
    "        \n",
    "    for k in model: grad_buffer[k] += {'W1':dW1, 'W2':dW2}[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-25 14:00:22,549] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#env.monitor.start('CartPole', force=True)\n",
    "observation = env.reset()\n",
    "reward_sum, episode_num = 0,0\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "score_queue = []\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-25 14:00:47,066] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 2, reward : 138.0, reward_mean : 138.0\n",
      "episode : 3, reward : 200.0, reward_mean : 158.666666667\n",
      "episode : 4, reward : 140.0, reward_mean : 154.0\n",
      "episode : 5, reward : 200.0, reward_mean : 163.2\n",
      "episode : 6, reward : 200.0, reward_mean : 169.333333333\n",
      "episode : 7, reward : 17.0, reward_mean : 147.571428571\n",
      "episode : 8, reward : 81.0, reward_mean : 139.25\n",
      "episode : 9, reward : 200.0, reward_mean : 146.0\n",
      "episode : 10, reward : 97.0, reward_mean : 141.1\n",
      "episode : 11, reward : 187.0, reward_mean : 145.272727273\n",
      "episode : 12, reward : 200.0, reward_mean : 149.833333333\n",
      "episode : 13, reward : 189.0, reward_mean : 152.846153846\n",
      "episode : 14, reward : 200.0, reward_mean : 156.214285714\n",
      "episode : 15, reward : 200.0, reward_mean : 159.133333333\n",
      "episode : 16, reward : 200.0, reward_mean : 161.6875\n",
      "episode : 17, reward : 128.0, reward_mean : 159.705882353\n",
      "episode : 18, reward : 139.0, reward_mean : 158.555555556\n",
      "episode : 19, reward : 200.0, reward_mean : 160.736842105\n",
      "episode : 20, reward : 200.0, reward_mean : 162.7\n",
      "episode : 21, reward : 200.0, reward_mean : 164.476190476\n",
      "episode : 22, reward : 200.0, reward_mean : 166.090909091\n",
      "episode : 23, reward : 178.0, reward_mean : 166.608695652\n",
      "episode : 24, reward : 198.0, reward_mean : 167.916666667\n",
      "episode : 25, reward : 199.0, reward_mean : 169.16\n",
      "episode : 26, reward : 147.0, reward_mean : 168.307692308\n",
      "episode : 27, reward : 200.0, reward_mean : 169.481481481\n",
      "episode : 28, reward : 200.0, reward_mean : 170.571428571\n",
      "episode : 29, reward : 187.0, reward_mean : 171.137931034\n",
      "episode : 30, reward : 200.0, reward_mean : 172.1\n",
      "episode : 31, reward : 139.0, reward_mean : 171.032258065\n",
      "episode : 32, reward : 200.0, reward_mean : 171.9375\n",
      "episode : 33, reward : 110.0, reward_mean : 170.060606061\n",
      "episode : 34, reward : 141.0, reward_mean : 169.205882353\n",
      "episode : 35, reward : 158.0, reward_mean : 168.885714286\n",
      "episode : 36, reward : 200.0, reward_mean : 169.75\n",
      "episode : 37, reward : 200.0, reward_mean : 170.567567568\n",
      "episode : 38, reward : 200.0, reward_mean : 171.342105263\n",
      "episode : 39, reward : 200.0, reward_mean : 172.076923077\n",
      "episode : 40, reward : 200.0, reward_mean : 172.775\n",
      "episode : 41, reward : 200.0, reward_mean : 173.43902439\n",
      "episode : 42, reward : 200.0, reward_mean : 174.071428571\n",
      "episode : 43, reward : 200.0, reward_mean : 174.674418605\n",
      "episode : 44, reward : 128.0, reward_mean : 173.613636364\n",
      "episode : 45, reward : 103.0, reward_mean : 172.044444444\n",
      "episode : 46, reward : 199.0, reward_mean : 172.630434783\n",
      "episode : 47, reward : 200.0, reward_mean : 173.212765957\n",
      "episode : 48, reward : 26.0, reward_mean : 170.145833333\n",
      "episode : 49, reward : 200.0, reward_mean : 170.755102041\n",
      "episode : 50, reward : 200.0, reward_mean : 171.34\n",
      "episode : 51, reward : 200.0, reward_mean : 171.901960784\n",
      "episode : 52, reward : 200.0, reward_mean : 172.442307692\n",
      "episode : 53, reward : 177.0, reward_mean : 172.528301887\n",
      "episode : 54, reward : 200.0, reward_mean : 173.037037037\n",
      "episode : 55, reward : 147.0, reward_mean : 172.563636364\n",
      "episode : 56, reward : 200.0, reward_mean : 173.053571429\n",
      "episode : 57, reward : 170.0, reward_mean : 173.0\n",
      "episode : 58, reward : 161.0, reward_mean : 172.793103448\n",
      "episode : 59, reward : 193.0, reward_mean : 173.13559322\n",
      "episode : 60, reward : 200.0, reward_mean : 173.583333333\n",
      "episode : 61, reward : 200.0, reward_mean : 174.016393443\n",
      "episode : 62, reward : 200.0, reward_mean : 174.435483871\n",
      "episode : 63, reward : 200.0, reward_mean : 174.841269841\n",
      "episode : 64, reward : 200.0, reward_mean : 175.234375\n",
      "episode : 65, reward : 137.0, reward_mean : 174.646153846\n",
      "episode : 66, reward : 200.0, reward_mean : 175.03030303\n",
      "episode : 67, reward : 39.0, reward_mean : 173.0\n",
      "episode : 68, reward : 200.0, reward_mean : 173.397058824\n",
      "episode : 69, reward : 200.0, reward_mean : 173.782608696\n",
      "episode : 70, reward : 200.0, reward_mean : 174.157142857\n",
      "episode : 71, reward : 200.0, reward_mean : 174.521126761\n",
      "episode : 72, reward : 200.0, reward_mean : 174.875\n",
      "episode : 73, reward : 137.0, reward_mean : 174.356164384\n",
      "episode : 74, reward : 200.0, reward_mean : 174.702702703\n",
      "episode : 75, reward : 200.0, reward_mean : 175.04\n",
      "episode : 76, reward : 200.0, reward_mean : 175.368421053\n",
      "episode : 77, reward : 200.0, reward_mean : 175.688311688\n",
      "episode : 78, reward : 200.0, reward_mean : 176.0\n",
      "episode : 79, reward : 200.0, reward_mean : 176.303797468\n",
      "episode : 80, reward : 127.0, reward_mean : 175.6875\n",
      "episode : 81, reward : 168.0, reward_mean : 175.592592593\n",
      "episode : 82, reward : 200.0, reward_mean : 175.890243902\n",
      "episode : 83, reward : 35.0, reward_mean : 174.192771084\n",
      "episode : 84, reward : 111.0, reward_mean : 173.44047619\n",
      "episode : 85, reward : 133.0, reward_mean : 172.964705882\n",
      "episode : 86, reward : 200.0, reward_mean : 173.279069767\n",
      "episode : 87, reward : 145.0, reward_mean : 172.954022989\n",
      "episode : 88, reward : 181.0, reward_mean : 173.045454545\n",
      "episode : 89, reward : 200.0, reward_mean : 173.348314607\n",
      "episode : 90, reward : 200.0, reward_mean : 173.644444444\n",
      "episode : 91, reward : 157.0, reward_mean : 173.461538462\n",
      "episode : 92, reward : 200.0, reward_mean : 173.75\n",
      "episode : 93, reward : 199.0, reward_mean : 174.021505376\n",
      "episode : 94, reward : 147.0, reward_mean : 173.734042553\n",
      "episode : 95, reward : 200.0, reward_mean : 174.010526316\n",
      "episode : 96, reward : 200.0, reward_mean : 174.28125\n",
      "episode : 97, reward : 200.0, reward_mean : 174.546391753\n",
      "episode : 98, reward : 200.0, reward_mean : 174.806122449\n",
      "episode : 99, reward : 129.0, reward_mean : 174.343434343\n",
      "episode : 100, reward : 200.0, reward_mean : 174.6\n",
      "episode : 101, reward : 137.0, reward_mean : 174.59\n",
      "episode : 102, reward : 200.0, reward_mean : 175.21\n",
      "episode : 103, reward : 167.0, reward_mean : 174.88\n",
      "episode : 104, reward : 200.0, reward_mean : 175.48\n",
      "episode : 105, reward : 200.0, reward_mean : 175.48\n",
      "episode : 106, reward : 200.0, reward_mean : 175.48\n",
      "episode : 107, reward : 200.0, reward_mean : 177.31\n",
      "episode : 108, reward : 200.0, reward_mean : 178.5\n",
      "episode : 109, reward : 200.0, reward_mean : 178.5\n",
      "episode : 110, reward : 112.0, reward_mean : 178.65\n",
      "episode : 111, reward : 195.0, reward_mean : 178.73\n",
      "episode : 112, reward : 200.0, reward_mean : 178.73\n",
      "episode : 113, reward : 200.0, reward_mean : 178.84\n",
      "episode : 114, reward : 200.0, reward_mean : 178.84\n",
      "episode : 115, reward : 200.0, reward_mean : 178.84\n",
      "episode : 116, reward : 200.0, reward_mean : 178.84\n",
      "episode : 117, reward : 200.0, reward_mean : 179.56\n",
      "episode : 118, reward : 200.0, reward_mean : 180.17\n",
      "episode : 119, reward : 200.0, reward_mean : 180.17\n",
      "episode : 120, reward : 200.0, reward_mean : 180.17\n",
      "episode : 121, reward : 200.0, reward_mean : 180.17\n",
      "episode : 122, reward : 200.0, reward_mean : 180.17\n",
      "episode : 123, reward : 200.0, reward_mean : 180.39\n",
      "episode : 124, reward : 200.0, reward_mean : 180.41\n",
      "episode : 125, reward : 200.0, reward_mean : 180.42\n",
      "episode : 126, reward : 200.0, reward_mean : 180.95\n",
      "episode : 127, reward : 200.0, reward_mean : 180.95\n",
      "episode : 128, reward : 200.0, reward_mean : 180.95\n",
      "episode : 129, reward : 200.0, reward_mean : 181.08\n",
      "episode : 130, reward : 200.0, reward_mean : 181.08\n",
      "episode : 131, reward : 200.0, reward_mean : 181.69\n",
      "episode : 132, reward : 200.0, reward_mean : 181.69\n",
      "episode : 133, reward : 200.0, reward_mean : 182.59\n",
      "episode : 134, reward : 200.0, reward_mean : 183.18\n",
      "episode : 135, reward : 200.0, reward_mean : 183.6\n",
      "episode : 136, reward : 200.0, reward_mean : 183.6\n",
      "episode : 137, reward : 200.0, reward_mean : 183.6\n",
      "episode : 138, reward : 200.0, reward_mean : 183.6\n",
      "episode : 139, reward : 200.0, reward_mean : 183.6\n",
      "episode : 140, reward : 200.0, reward_mean : 183.6\n",
      "episode : 141, reward : 200.0, reward_mean : 183.6\n",
      "episode : 142, reward : 200.0, reward_mean : 183.6\n",
      "episode : 143, reward : 200.0, reward_mean : 183.6\n",
      "episode : 144, reward : 200.0, reward_mean : 184.32\n",
      "episode : 145, reward : 200.0, reward_mean : 185.29\n",
      "episode : 146, reward : 200.0, reward_mean : 185.3\n",
      "episode : 147, reward : 200.0, reward_mean : 185.3\n",
      "episode : 148, reward : 200.0, reward_mean : 187.04\n",
      "episode : 149, reward : 200.0, reward_mean : 187.04\n",
      "episode : 150, reward : 200.0, reward_mean : 187.04\n",
      "episode : 151, reward : 200.0, reward_mean : 187.04\n",
      "episode : 152, reward : 200.0, reward_mean : 187.04\n",
      "episode : 153, reward : 200.0, reward_mean : 187.27\n",
      "episode : 154, reward : 200.0, reward_mean : 187.27\n",
      "episode : 155, reward : 200.0, reward_mean : 187.8\n",
      "episode : 156, reward : 200.0, reward_mean : 187.8\n",
      "episode : 157, reward : 200.0, reward_mean : 188.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 158, reward : 200.0, reward_mean : 188.49\n",
      "episode : 159, reward : 200.0, reward_mean : 188.56\n",
      "episode : 160, reward : 200.0, reward_mean : 188.56\n",
      "episode : 161, reward : 200.0, reward_mean : 188.56\n",
      "episode : 162, reward : 200.0, reward_mean : 188.56\n",
      "episode : 163, reward : 200.0, reward_mean : 188.56\n",
      "episode : 164, reward : 200.0, reward_mean : 188.56\n",
      "episode : 165, reward : 200.0, reward_mean : 189.19\n",
      "episode : 166, reward : 200.0, reward_mean : 189.19\n",
      "episode : 167, reward : 200.0, reward_mean : 190.8\n",
      "episode : 168, reward : 200.0, reward_mean : 190.8\n",
      "episode : 169, reward : 200.0, reward_mean : 190.8\n",
      "episode : 170, reward : 200.0, reward_mean : 190.8\n",
      "episode : 171, reward : 200.0, reward_mean : 190.8\n",
      "episode : 172, reward : 200.0, reward_mean : 190.8\n",
      "episode : 173, reward : 200.0, reward_mean : 191.43\n",
      "episode : 174, reward : 200.0, reward_mean : 191.43\n",
      "episode : 175, reward : 200.0, reward_mean : 191.43\n",
      "episode : 176, reward : 200.0, reward_mean : 191.43\n",
      "episode : 177, reward : 200.0, reward_mean : 191.43\n",
      "episode : 178, reward : 200.0, reward_mean : 191.43\n",
      "episode : 179, reward : 200.0, reward_mean : 191.43\n",
      "episode : 180, reward : 200.0, reward_mean : 192.16\n",
      "episode : 181, reward : 200.0, reward_mean : 192.48\n",
      "episode : 182, reward : 200.0, reward_mean : 192.48\n",
      "episode : 183, reward : 200.0, reward_mean : 194.13\n",
      "episode : 184, reward : 200.0, reward_mean : 195.02\n",
      "episode : 185, reward : 200.0, reward_mean : 195.69\n",
      "episode : 186, reward : 200.0, reward_mean : 195.69\n",
      "episode : 187, reward : 200.0, reward_mean : 196.24\n",
      "episode : 188, reward : 200.0, reward_mean : 196.43\n",
      "episode : 189, reward : 200.0, reward_mean : 196.43\n",
      "episode : 190, reward : 200.0, reward_mean : 196.43\n",
      "episode : 191, reward : 200.0, reward_mean : 196.86\n",
      "episode : 192, reward : 200.0, reward_mean : 196.86\n",
      "episode : 193, reward : 200.0, reward_mean : 196.87\n",
      "episode : 194, reward : 200.0, reward_mean : 197.4\n",
      "episode : 195, reward : 200.0, reward_mean : 197.4\n",
      "episode : 196, reward : 200.0, reward_mean : 197.4\n",
      "episode : 197, reward : 200.0, reward_mean : 197.4\n",
      "episode : 198, reward : 200.0, reward_mean : 197.4\n",
      "episode : 199, reward : 200.0, reward_mean : 198.11\n",
      "episode : 200, reward : 200.0, reward_mean : 198.11\n",
      "episode : 201, reward : 200.0, reward_mean : 198.74\n",
      "episode : 202, reward : 200.0, reward_mean : 198.74\n",
      "episode : 203, reward : 200.0, reward_mean : 199.07\n",
      "episode : 204, reward : 200.0, reward_mean : 199.07\n",
      "episode : 205, reward : 200.0, reward_mean : 199.07\n",
      "episode : 206, reward : 200.0, reward_mean : 199.07\n",
      "episode : 207, reward : 200.0, reward_mean : 199.07\n",
      "episode : 208, reward : 200.0, reward_mean : 199.07\n",
      "episode : 209, reward : 200.0, reward_mean : 199.07\n",
      "episode : 210, reward : 200.0, reward_mean : 199.95\n",
      "episode : 211, reward : 200.0, reward_mean : 200.0\n",
      "CartPole solved!!!!!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    x = prepro(observation)\n",
    "    \n",
    "    act_prob, h = policy_forward(x)\n",
    "    \n",
    "    if np.mean(score_queue) > 180:\n",
    "        action = 1 if 0.5 < act_prob else 0\n",
    "    else:\n",
    "        action = 1 if np.random.uniform() < act_prob else 0\n",
    "\n",
    "    xs.append(x)\n",
    "    hs.append(h)\n",
    "    y = action\n",
    "    dlogps.append(y - act_prob)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    drs.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        episode_num += 1\n",
    "        \n",
    "        if episode_num > score_queue_size:\n",
    "            score_queue.append(reward_sum)\n",
    "            score_queue.pop(0)\n",
    "        else:\n",
    "            score_queue.append(reward_sum)\n",
    "        \n",
    "        print \"episode : \" + str(episode_num) + \", reward : \" + str(reward_sum) + \", reward_mean : \" + str(np.mean(score_queue))\n",
    "        rList.append(reward_sum)\n",
    "        if np.mean(score_queue) >= 200:\n",
    "            print \"CartPole solved!!!!!\"\n",
    "            break\n",
    "        \n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "        xs,hs,dlogps,drs = [],[],[],[]\n",
    "        \n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "        \n",
    "        epdlogp *= discounted_epr\n",
    "        \n",
    "        policy_backward(eph,epdlogp,epx)\n",
    "        for k,v in model.iteritems():\n",
    "            g = grad_buffer[k] \n",
    "            rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate)*g**2\n",
    "            model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "            grad_buffer[k] = np.zeros_like(v)\n",
    "        \n",
    "        if episode_num % 1000 == 0: pickle.dump(model, open('Cart.p', 'wb'))\n",
    "        \n",
    "        reward_sum = 0\n",
    "        observation = env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2a136658b8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/howoongjun/anaconda2/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mbar\u001b[0;34m(left, height, width, bottom, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   2702\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m         ret = ax.bar(left, height, width=width, bottom=bottom, data=data,\n\u001b[0;32m-> 2704\u001b[0;31m                      **kwargs)\n\u001b[0m\u001b[1;32m   2705\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2706\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/howoongjun/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/howoongjun/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, left, height, width, bottom, **kwargs)\u001b[0m\n\u001b[1;32m   2118\u001b[0m                 \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m                 \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2120\u001b[0;31m                 \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_nolegend_'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2121\u001b[0m                 )\n\u001b[1;32m   2122\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/howoongjun/anaconda2/lib/python2.7/site-packages/matplotlib/patches.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xy, width, height, angle, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_angle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;31m# Note: This cannot be calculated until this is added to an Axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number"
     ]
    }
   ],
   "source": [
    "plt.bar(range(len(rList)), rList, color = \"blue\", width = 0.01)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
